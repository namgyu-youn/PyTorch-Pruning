# PyTorch-Pruning

This project aims to **compare variable pruning** techniques for PyTorch based models. For deepen understanding, we uses multiple-metrics (accuracy & latency), [torch.profiler](https://docs.pytorch.org/tutorials/recipes/recipes/profiler_recipe.html), etc.

## References

[1] TinyML and Efficient Deep Learning Computing (MIT-6.5940, [Lecture](https://hanlab.mit.edu/courses/2024-fall-65940))

[2] A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations (IEEE'24, [arXiv](https://arxiv.org/abs/2308.06767))

[3] Pruning Deep Neural Networks from a Sparsity Perspective (ICLR'23, [arXiv](https://arxiv.org/abs/2302.05601))

[4] APT: Adaptive Pruning and Tuning Pretrained LLM for Efficient Training and Inference (ICML'24, [presentation](https://icml.cc/virtual/2024/oral/35453))

[5] Fluctuation-based Adaptive Structured Pruning for Large Language Models (AAAI'24)

[6] Isomorphic Pruning for Vision Models (ECCV'24)

[7] How Well Do Sparse ImageNet Models Transfer? (CVPR'22, )

[8] A Simple and Effective Pruning Approach for Large Language Models (ICLR'24, [arXiv](https://arxiv.org/abs/2306.11695))

[9] How to Quantize Transformer-based model for TensorRT Deployment ([SqueezeBits - Blog](https://blog.squeezebits.com/how-to-quantize-transformerbased-model-for-tensorrt-deployment-55802))